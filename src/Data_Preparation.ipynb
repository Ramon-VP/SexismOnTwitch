{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Preparation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qch_uFvOoQa-"
      },
      "source": [
        "#### Initialize Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHusJs-g-d71"
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import itertools\n",
        "\n",
        "data_path = ''\n",
        "## Uncoment if using google colab\n",
        "# from google.colab import drive\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# drive.mount('/content/drive') \n",
        "# data_path = '/content/drive/My Drive/Data/'\n",
        "dataset_path = data_path+'Datasets/'\n",
        "cleanDataset_path = data_path+'CleanDatasets/'\n",
        "categorizedDataset_path = data_path+'CategorizedDatasets/'\n",
        "train_path = data_path+'Train/'\n",
        "test_path = data_path+'Test/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYwTJMOR_E2f"
      },
      "source": [
        "#### Clean Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anmUH_WAYrD6",
        "outputId": "40f6f00a-df6e-4488-b994-48c383a78f41"
      },
      "source": [
        "def repeated_chars(word):\n",
        "    clean_word=\"\"\n",
        "    for _, dups in (itertools.groupby(word)):\n",
        "        seq = list(dups)\n",
        "        clean_word += seq[0]\n",
        "        if len(seq)>1:\n",
        "            clean_word += seq[1]\n",
        "    return clean_word\n",
        "    \n",
        "saludos = ['hola','ola','hhola','hoola','holaa', 'holi','holii','hey','heyy','ei','halo','saludos','adios','adioos','aadios','adiios','adioss']\n",
        "def cleanComment(comment):\n",
        "    stemming = PorterStemmer()\n",
        "    stops = set(stopwords.words(\"spanish\"))\n",
        "    ## START CODE\n",
        "    line= comment.lower() ## Transform in lowercase\n",
        "    line=re.sub('[:\\[\\]&%$\\\"\\'!./,;:?=¿^\\-#_*+)<>(¡@]','',line)\n",
        "    line= line.split() ## Tokenize the text to get a list of terms\n",
        "    line= [repeated_chars(word) for word in line]\n",
        "    line= [word for word in line if word not in saludos]  ##eliminate the stopwords (HINT: use List Comprehension)\n",
        "    line= [stemming.stem(word) for word in line] ## perform stemming (HINT: use List Comprehension)\n",
        "    ## END CODE\n",
        "    cleanedComment = \"\"\n",
        "    for i, word in enumerate(line):\n",
        "      if i!=0:\n",
        "        cleanedComment += \" \"\n",
        "      cleanedComment += word\n",
        "    return cleanedComment\n",
        "\n",
        "print(cleanComment(\"holaa guapa\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "guapa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP5w86GF_Dtt"
      },
      "source": [
        "## LOAD ALL COMMENTS\n",
        "\n",
        "fp = open(categorizedDataset_path+\"Chat_shainny__0.123.txt\", \"r\", encoding=\"utf-8\")    \n",
        "\n",
        "inappropriate_comments = list()\n",
        "sexist_comments = list()\n",
        "other_comments = list()\n",
        "\n",
        "line = fp.readline()\n",
        "while(len(line) != 0):\n",
        "    comment, category = line.split(';')\n",
        "    if category[0]=='0':\n",
        "        other_comments.append(cleanComment(comment))\n",
        "    elif category[0]=='1':\n",
        "        sexist_comments.append(cleanComment(comment))\n",
        "    elif category[0]=='2':\n",
        "        inappropriate_comments.append(cleanComment(comment))\n",
        "    line = fp.readline()\n",
        "fp.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOGk36EqKIvu"
      },
      "source": [
        "fpw = open(train_path+\"Chat_Training_Model2.txt\", \"w\", encoding=\"utf-8\")\n",
        "\n",
        "# for i, comm in enumerate(other_comments):\n",
        "#     if i>500:\n",
        "#       break\n",
        "#     fpw.write(\"__label__0 \"+comm+'\\n')\n",
        "\n",
        "for comm in inappropriate_comments:\n",
        "  fpw.write(\"__label__0 \"+comm+'\\n')  \n",
        "\n",
        "for comm in sexist_comments:\n",
        "  fpw.write(\"__label__1 \"+comm+'\\n')  \n",
        "\n",
        "fpw.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}